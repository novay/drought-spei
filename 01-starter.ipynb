{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading & Visualizing the Data**\n",
    "\n",
    "In this section, we load the training and validation data into numpy arrays and visualize the drought classes and meteorological attributes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the json files for training, validation and testing into the `data` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "files = {}\n",
    "\n",
    "for dirname, _, filenames in os.walk('/data'):\n",
    "    for filename in filenames:\n",
    "        if 'train' in filename:\n",
    "            files['train'] = os.path.join(dirname, filename)\n",
    "        if 'valid' in filename:\n",
    "            files['valid'] = os.path.join(dirname, filename)\n",
    "        if 'test' in filename:\n",
    "            files['test'] = os.path.join(dirname, filename)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following classes exist, ranging from no drought (None), to extreme drought (D4). This could be treated as a regression, ordinal or classification problem, but for now we will treat it as 5 distinct classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class2id = {\n",
    "    'None': 0,\n",
    "    'D0': 1,\n",
    "    'D1': 2,\n",
    "    'D2': 3,\n",
    "    'D3': 4,\n",
    "    'D4': 5,\n",
    "}\n",
    "\n",
    "id2class = {v: k for k, v in class2id.items()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll define a helper method to load the datasets. This just walks through the json and discards the few samples that are corrupted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {\n",
    "    k: pd.read_csv(files[k]).set_index(['fips', 'date'])\n",
    "    for k in files.keys()\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also add a helper function to interpolate the drought values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_nans(padata, pkind='linear'):\n",
    "    \"\"\"\n",
    "    see: https://stackoverflow.com/a/53050216/2167159\n",
    "    \"\"\"\n",
    "    aindexes = np.arange(padata.shape[0])\n",
    "    agood_indexes, = np.where(np.isfinite(padata))\n",
    "    f = interp1d(agood_indexes\n",
    "               , padata[agood_indexes]\n",
    "               , bounds_error=False\n",
    "               , copy=False\n",
    "               , fill_value=\"extrapolate\"\n",
    "               , kind=pkind)\n",
    "    return f(aindexes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encode the day of year using sin/cos and add the data loading function `loadXY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_encode(date):\n",
    "    if isinstance(date, str):\n",
    "        date = datetime.strptime(date, \"%Y-%m-%d\")\n",
    "    return (\n",
    "        np.sin(2 * np.pi * date.timetuple().tm_yday / 366),\n",
    "        np.cos(2 * np.pi * date.timetuple().tm_yday / 366),\n",
    "    )\n",
    "\n",
    "def loadXY(\n",
    "    df,\n",
    "    random_state=42, # keep this at 42\n",
    "    window_size=180, # how many days in the past (default/competition: 180)\n",
    "    target_size=6, # how many weeks into the future (default/competition: 6)\n",
    "    fuse_past=True, # add the past drought observations? (default: True)\n",
    "    return_fips=False, # return the county identifier (do not use for predictions)\n",
    "    encode_season=True, # encode the season using the function above (default: True) \n",
    "    use_prev_year=False, # add observations from 1 year prior?\n",
    "):\n",
    "    df = dfs[df]\n",
    "    soil_df = pd.read_csv(\"/kaggle/input/soil_data.csv\")\n",
    "    time_data_cols = sorted(\n",
    "        [c for c in df.columns if c not in [\"fips\", \"date\", \"score\"]]\n",
    "    )\n",
    "    static_data_cols = sorted(\n",
    "        [c for c in soil_df.columns if c not in [\"soil\", \"lat\", \"lon\"]]\n",
    "    )\n",
    "    count = 0\n",
    "    score_df = df.dropna(subset=[\"score\"])\n",
    "    X_static = np.empty((len(df) // window_size, len(static_data_cols)))\n",
    "    X_fips_date = []\n",
    "    add_dim = 0\n",
    "    if use_prev_year:\n",
    "        add_dim += len(time_data_cols)\n",
    "    if fuse_past:\n",
    "        add_dim += 1\n",
    "        if use_prev_year:\n",
    "            add_dim += 1\n",
    "    if encode_season:\n",
    "        add_dim += 2\n",
    "    X_time = np.empty(\n",
    "        (len(df) // window_size, window_size, len(time_data_cols) + add_dim)\n",
    "    )\n",
    "    y_past = np.empty((len(df) // window_size, window_size))\n",
    "    y_target = np.empty((len(df) // window_size, target_size))\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    for fips in tqdm(score_df.index.get_level_values(0).unique()):\n",
    "        if random_state is not None:\n",
    "            start_i = np.random.randint(1, window_size)\n",
    "        else:\n",
    "            start_i = 1\n",
    "        fips_df = df[(df.index.get_level_values(0) == fips)]\n",
    "        X = fips_df[time_data_cols].values\n",
    "        y = fips_df[\"score\"].values\n",
    "        X_s = soil_df[soil_df[\"fips\"] == fips][static_data_cols].values[0]\n",
    "        for i in range(start_i, len(y) - (window_size + target_size * 7), window_size):\n",
    "            X_fips_date.append((fips, fips_df.index[i : i + window_size][-1]))\n",
    "            X_time[count, :, : len(time_data_cols)] = X[i : i + window_size]\n",
    "            if use_prev_year:\n",
    "                if i < 365 or len(X[i - 365 : i + window_size - 365]) < window_size:\n",
    "                    continue\n",
    "                X_time[count, :, -len(time_data_cols) :] = X[\n",
    "                    i - 365 : i + window_size - 365\n",
    "                ]\n",
    "            if not fuse_past:\n",
    "                y_past[count] = interpolate_nans(y[i : i + window_size])\n",
    "            else:\n",
    "                X_time[count, :, len(time_data_cols)] = interpolate_nans(\n",
    "                    y[i : i + window_size]\n",
    "                )\n",
    "            if encode_season:\n",
    "                enc_dates = [\n",
    "                    date_encode(d) for f, d in fips_df.index[i : i + window_size].values\n",
    "                ]\n",
    "                d_sin, d_cos = [s for s, c in enc_dates], [c for s, c in enc_dates]\n",
    "                X_time[count, :, len(time_data_cols) + (add_dim - 2)] = d_sin\n",
    "                X_time[count, :, len(time_data_cols) + (add_dim - 2) + 1] = d_cos\n",
    "            temp_y = y[i + window_size : i + window_size + target_size * 7]\n",
    "            y_target[count] = np.array(temp_y[~np.isnan(temp_y)][:target_size])\n",
    "            X_static[count] = X_s\n",
    "            count += 1\n",
    "    print(f\"loaded {count} samples\")\n",
    "    results = [X_static[:count], X_time[:count], y_target[:count]]\n",
    "    if not fuse_past:\n",
    "        results.append(y_past[:count])\n",
    "    if return_fips:\n",
    "        results.append(X_fips_date)\n",
    "    return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we add a helper to normalise the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_dict = {}\n",
    "scaler_dict_static = {}\n",
    "scaler_dict_past = {}\n",
    "\n",
    "\n",
    "def normalize(X_static, X_time, y_past=None, fit=False):\n",
    "    for index in tqdm(range(X_time.shape[-1])):\n",
    "        if fit:\n",
    "            scaler_dict[index] = RobustScaler().fit(X_time[:, :, index].reshape(-1, 1))\n",
    "        X_time[:, :, index] = (\n",
    "            scaler_dict[index]\n",
    "            .transform(X_time[:, :, index].reshape(-1, 1))\n",
    "            .reshape(-1, X_time.shape[-2])\n",
    "        )\n",
    "    for index in tqdm(range(X_static.shape[-1])):\n",
    "        if fit:\n",
    "            scaler_dict_static[index] = RobustScaler().fit(\n",
    "                X_static[:, index].reshape(-1, 1)\n",
    "            )\n",
    "        X_static[:, index] = (\n",
    "            scaler_dict_static[index]\n",
    "            .transform(X_static[:, index].reshape(-1, 1))\n",
    "            .reshape(1, -1)\n",
    "        )\n",
    "    index = 0\n",
    "    if y_past is not None:\n",
    "        if fit:\n",
    "            scaler_dict_past[index] = RobustScaler().fit(y_past.reshape(-1, 1))\n",
    "        y_past[:, :] = (\n",
    "            scaler_dict_past[index]\n",
    "            .transform(y_past.reshape(-1, 1))\n",
    "            .reshape(-1, y_past.shape[-1])\n",
    "        )\n",
    "        return X_static, X_time, y_past\n",
    "    return X_static, X_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X_static_train, X_time_train, y_target_train \u001b[39m=\u001b[39m loadXY(\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mtrain shape\u001b[39m\u001b[39m\"\u001b[39m, X_time_train\u001b[39m.\u001b[39mshape)\n\u001b[1;32m      3\u001b[0m X_static_valid, X_time_valid, y_target_valid \u001b[39m=\u001b[39m loadXY(\u001b[39m\"\u001b[39m\u001b[39mvalid\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 19\u001b[0m, in \u001b[0;36mloadXY\u001b[0;34m(df, random_state, window_size, target_size, fuse_past, return_fips, encode_season, use_prev_year)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mloadXY\u001b[39m(\n\u001b[1;32m     10\u001b[0m     df,\n\u001b[1;32m     11\u001b[0m     random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m, \u001b[39m# keep this at 42\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m     use_prev_year\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m# add observations from 1 year prior?\u001b[39;00m\n\u001b[1;32m     18\u001b[0m ):\n\u001b[0;32m---> 19\u001b[0m     df \u001b[39m=\u001b[39m dfs[df]\n\u001b[1;32m     20\u001b[0m     soil_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m\"\u001b[39m\u001b[39m/kaggle/input/soil_data.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m     time_data_cols \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(\n\u001b[1;32m     22\u001b[0m         [c \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m df\u001b[39m.\u001b[39mcolumns \u001b[39mif\u001b[39;00m c \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mfips\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mdate\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mscore\u001b[39m\u001b[39m\"\u001b[39m]]\n\u001b[1;32m     23\u001b[0m     )\n",
      "\u001b[0;31mKeyError\u001b[0m: 'train'"
     ]
    }
   ],
   "source": [
    "X_static_train, X_time_train, y_target_train = loadXY(\"train\")\n",
    "print(\"train shape\", X_time_train.shape)\n",
    "X_static_valid, X_time_valid, y_target_valid = loadXY(\"valid\")\n",
    "print(\"validation shape\", X_time_valid.shape)\n",
    "X_static_train, X_time_train = normalize(X_static_train, X_time_train, fit=True)\n",
    "X_static_valid, X_time_valid = normalize(X_static_valid, X_time_valid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
